{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb1c70e-fda2-4cc8-a72e-372bd4e5e611",
   "metadata": {},
   "source": [
    "# Bayes By Backprop\n",
    "\n",
    "* [**Paper**](https://arxiv.org/abs/1505.05424)\n",
    "\n",
    "In this paper, the authors describe a method for weight regularisation which provides direct information on the uncertainty of the weights. In essence, we try to approximate the posterior $p(w | D)$ of weights $w$, given the data $D$.\n",
    "\n",
    "From Bayes' theorem, we know:\n",
    "\n",
    "$$\n",
    "p(w | D) = \\frac{p(D | w)p(w)}{p(D)}\n",
    "$$\n",
    "\n",
    "However, estimating $p(w | D)$ directly is intractible due to $p(D)$ (the evidence). So instead we can approximate $p(w | D)$ with the variational posterior $q(w | \\theta)$. The approximation can be done by selecting parameters $\\theta$ such that the Kullback-Leibler divergence between the two distributions is minimised:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\text{argmin}_{\\theta} KL[q(w | \\theta) || p(w | D)]\n",
    "$$\n",
    "\n",
    "The divergence can be re-written into:\n",
    "\n",
    "$$\\begin{align}\n",
    "KL[q(w | \\theta) || p(w | D)] &= \\int q(w | \\theta) \\log \\frac{q(w | \\theta)}{p(w | D)} \\\\\n",
    "&= \\int q(w | \\theta) \\log \\frac{q(w | \\theta) p(D)}{p(D | w) p(w)} \\\\\n",
    "&= \\int q(w | \\theta) \\log \\frac{q(w | \\theta)}{p(D | w) p(w)} + \\mathbb{E}_{q(w | \\theta)}[\\log p(D)] \\\\\n",
    "&= \\int q(w | \\theta) \\log \\frac{q(w | \\theta)}{p(D | w) p(w)} + \\log p(D)\n",
    "\\end{align}$$\n",
    "\n",
    "Since $p(D)$ is constant in this expression and does not depend on $\\theta$, we can ignore it in the minimisation. This reduces the expression to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin}_{\\theta} KL[q(w | \\theta) || p(w | D)] &= \\text{argmin}_{\\theta} \\int q(w | \\theta) \\log \\frac{q(w | \\theta)}{p(D | w) p(w)} \\\\\n",
    "&= \\text{argmin}_{\\theta} \\int q(w | \\theta) \\log \\frac{q(w | \\theta)}{p(w)} - \\int q(w | \\theta) \\log p(D | w) \\\\\n",
    "&= \\text{argmin}_{\\theta} KL[q(w | \\theta) || p(w)] - \\mathbb{E}_{q(w | \\theta)}[\\log p(D | w)]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af4939c-0f1c-4fde-998c-5691d6c6e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860a8f9b-2b7c-46dd-8be4-d4da7470d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65eb5832-9f4a-4e8c-8e12-14f1dceb8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = jnp.array(X_train.reshape(len(X_train), -1) / 126)\n",
    "X_test = jnp.array(X_test.reshape(len(X_test), -1) / 126)\n",
    "\n",
    "y_train = jax.nn.one_hot(y_train, 10)\n",
    "y_test = jax.nn.one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572ae00c-c4e6-42c6-ba4c-17c494fe613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_sigma(n):\n",
    "    return 2 / n\n",
    "\n",
    "\n",
    "def inv_t(sigma):\n",
    "    return jnp.log(jnp.exp(sigma) - 1)\n",
    "\n",
    "\n",
    "def init_mu(shape, rng):\n",
    "    return 0.1 * jax.random.normal(rng, shape)\n",
    "\n",
    "\n",
    "def init_rho(shape, rng):\n",
    "    return inv_t(kaiming_sigma(shape[-1])) + 0.1 * jax.random.normal(rng, shape)\n",
    "\n",
    "\n",
    "def init_theta(shape, rng):\n",
    "    a, b = jax.random.split(rng)\n",
    "    return (init_mu(shape, a), init_rho(shape, b))\n",
    "\n",
    "\n",
    "def init_Wb(shape, rng):\n",
    "    a, b = jax.random.split(rng)\n",
    "    return (init_theta(shape, a), init_theta(shape[-1:], b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c78c4a9-d2b4-462e-af88-48fb1fac58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = Tuple[jnp.ndarray, jnp.ndarray]\n",
    "Params = Tuple[Theta, ...]\n",
    "\n",
    "\n",
    "def sample_w(mu: jnp.ndarray, rho: jnp.ndarray, rng_key: jnp.ndarray) -> jnp.ndarray:\n",
    "    eps: jnp.ndarray = jax.random.normal(rng_key, mu.shape)\n",
    "    w = mu + jnp.log(1 + jnp.exp(rho)) * eps\n",
    "    return w\n",
    "\n",
    "\n",
    "def bbb_mlp(params: Params, X: jnp.ndarray, rng_key: jnp.ndarray) -> Tuple[jnp.ndarray, Tuple[jnp.ndarray, ...]]:\n",
    "    theta_W0, theta_b0, theta_W1, theta_b1, theta_W2, theta_b2 = params\n",
    "    k0, k1, k2, k3, k4, k5 = jax.random.split(rng_key, 6)\n",
    "\n",
    "    W0 = sample_w(*theta_W0, k0)\n",
    "    b0 = sample_w(*theta_b0, k1)\n",
    "    \n",
    "    W1 = sample_w(*theta_W1, k2)\n",
    "    b1 = sample_w(*theta_b1, k3)\n",
    "    \n",
    "    W2 = sample_w(*theta_W2, k4)\n",
    "    b2 = sample_w(*theta_b2, k5)\n",
    "    return nn.relu(nn.relu(X @ W0 + b0) @ W1 + b1) @ W2 + b2, (W0, W1, W2)\n",
    "\n",
    "\n",
    "def kl_div(p: Params) -> jnp.ndarray:\n",
    "    kl = jnp.array(0)\n",
    "    \n",
    "    mu_p, sigma_p = jnp.array(0), jnp.exp(-2)\n",
    "    for (mu_q, rho_q) in p:\n",
    "        sigma_q = jnp.log(1 + jnp.exp(rho_q))\n",
    "        kl += jnp.sum(\n",
    "            2 * jnp.log(sigma_p / sigma_q)\n",
    "            - 1 + (sigma_q / sigma_p) ** 2\n",
    "            + ((mu_p - mu_q) / sigma_p) ** 2\n",
    "        )\n",
    "    return 0.5 * kl\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    params: Params,\n",
    "    X: jnp.ndarray,\n",
    "    y: jnp.ndarray,\n",
    "    rng_key: jnp.ndarray,\n",
    "    n_posterior_samples: int = 10,\n",
    "    eta: float = 1e-3,\n",
    "    beta: float = 0.05,\n",
    ") -> Tuple[Params, jnp.ndarray, jnp.ndarray]:\n",
    "    def loss_fn(p: Params, k: jnp.ndarray) -> jnp.ndarray:\n",
    "        y_hat, _ = bbb_mlp(p, X, k)\n",
    "        loss = (\n",
    "            # log q(w | theta) / p(w)\n",
    "            beta * kl_div(p)\n",
    "            # log p(D | theta)\n",
    "            - jnp.mean(\n",
    "                jnp.sum(y * nn.log_softmax(y_hat, axis=-1), axis=-1)\n",
    "            )\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    G = jax.tree_map(lambda _: jnp.zeros_like(_), params)\n",
    "    f = jax.value_and_grad(loss_fn)\n",
    "    L = jnp.array(0)\n",
    "    for i in range(n_posterior_samples):\n",
    "        rng_key, key = jax.random.split(rng_key)\n",
    "        l, g = f(params, key)\n",
    "        L += l\n",
    "        G = jax.tree_map(lambda c, k: c + k, g, G)\n",
    "\n",
    "    # even though we are trying to approximate the expectation of the gradient\n",
    "    # there is no point in normalising by the number of samples, as it would be\n",
    "    # equivalent to using a lower learning rate\n",
    "    # G = jax.tree_map(lambda g: g / n_posterior_samples, G)\n",
    "    return update_params(params, G, eta=eta), L / n_posterior_samples, rng_key\n",
    "\n",
    "\n",
    "def update_params(params: Params, gradients: Params, eta: float) -> Params:\n",
    "    return jax.tree_map(lambda w, g: w - eta * g, params, gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95505b33-f5cb-4a87-9987-3902e692c48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch_indices(rng: jnp.ndarray, dataset_size: int, batch_size: int) -> jnp.ndarray:\n",
    "    steps_per_epoch = dataset_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, dataset_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    return perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4080d2-8d78-4d66-8765-4758d36a858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c9cd50-2fb6-424d-bbe7-057127f41961",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = ()\n",
    "for l in [\n",
    "    (28 ** 2, 512),\n",
    "    (512, 256),\n",
    "    (256, 10),\n",
    "]:\n",
    "    p0 = p0 + init_Wb(l, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710b811d-2abb-47e0-bb9f-73b675a31830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_beta(I):\n",
    "    M = len(I)\n",
    "    for ix, i in enumerate(I, 1):\n",
    "        yield (2 ** (M - ix)) / (2 ** M - 1), i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24257728-b112-48dd-9adc-e8945c875b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 (loss=0.982): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:45<00:00, 10.28it/s]\n",
      "Epoch 1 (loss=0.794): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.70it/s]\n",
      "Epoch 2 (loss=0.588): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 16.11it/s]\n",
      "Epoch 3 (loss=0.664): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:28<00:00, 16.32it/s]\n",
      "Epoch 4 (loss=0.805): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.83it/s]\n",
      "Epoch 5 (loss=0.755): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:28<00:00, 16.31it/s]\n",
      "Epoch 6 (loss=0.723): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "Epoch 7 (loss=0.707): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.79it/s]\n",
      "Epoch 8 (loss=0.754): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.94it/s]\n",
      "Epoch 9 (loss=0.832): 100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.75it/s]\n",
      "Epoch 10 (loss=0.792): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.69it/s]\n",
      "Epoch 11 (loss=0.679): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 14.84it/s]\n",
      "Epoch 12 (loss=0.708): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.20it/s]\n",
      "Epoch 13 (loss=0.694): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:28<00:00, 16.18it/s]\n",
      "Epoch 14 (loss=0.634): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.71it/s]\n",
      "Epoch 15 (loss=0.678): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.71it/s]\n",
      "Epoch 16 (loss=0.684): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.50it/s]\n",
      "Epoch 17 (loss=0.749): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.59it/s]\n",
      "Epoch 18 (loss=0.734): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.88it/s]\n",
      "Epoch 19 (loss=0.779): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 15.01it/s]\n",
      "Epoch 20 (loss=0.612): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.16it/s]\n",
      "Epoch 21 (loss=0.599): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.49it/s]\n",
      "Epoch 22 (loss=0.762): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.51it/s]\n",
      "Epoch 23 (loss=0.728): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.66it/s]\n",
      "Epoch 24 (loss=0.683): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.66it/s]\n",
      "Epoch 25 (loss=0.737): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.68it/s]\n",
      "Epoch 26 (loss=0.776): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.54it/s]\n",
      "Epoch 27 (loss=0.597): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.42it/s]\n",
      "Epoch 28 (loss=0.777): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:32<00:00, 14.54it/s]\n",
      "Epoch 29 (loss=0.709): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.16it/s]\n",
      "Epoch 30 (loss=0.693): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 14.96it/s]\n",
      "Epoch 31 (loss=0.686): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:32<00:00, 14.62it/s]\n",
      "Epoch 32 (loss=0.658): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.24it/s]\n",
      "Epoch 33 (loss=0.760): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 14.94it/s]\n",
      "Epoch 34 (loss=0.683): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 15.05it/s]\n",
      "Epoch 35 (loss=0.538): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.69it/s]\n",
      "Epoch 36 (loss=0.633): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:29<00:00, 15.63it/s]\n",
      "Epoch 37 (loss=0.691): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 15.06it/s]\n",
      "Epoch 38 (loss=0.663): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:32<00:00, 14.59it/s]\n",
      "Epoch 39 (loss=0.831): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.32it/s]\n",
      "Epoch 40 (loss=0.699): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 14.88it/s]\n",
      "Epoch 41 (loss=0.672): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.24it/s]\n",
      "Epoch 42 (loss=0.716): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.24it/s]\n",
      "Epoch 43 (loss=0.692): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:31<00:00, 15.01it/s]\n",
      "Epoch 44 (loss=0.841): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.13it/s]\n",
      "Epoch 45 (loss=0.689): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.35it/s]\n",
      "Epoch 46 (loss=0.576): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.23it/s]\n",
      "Epoch 47 (loss=0.685): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.27it/s]\n",
      "Epoch 48 (loss=0.688): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.24it/s]\n",
      "Epoch 49 (loss=0.758): 100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:30<00:00, 15.25it/s]\n"
     ]
    }
   ],
   "source": [
    "r = rng\n",
    "p = p0\n",
    "for e in range(50):\n",
    "    r, r0 = jax.random.split(r, 2)\n",
    "    ix = get_batch_indices(r0, len(X_train), 128)\n",
    "    I = tqdm.tqdm(ix, desc=f\"Epoch {e}\")\n",
    "    for beta, i in with_beta(I):\n",
    "        p, l, r = train_step(p, X_train[i], y_train[i], r, eta=1e-3, beta=beta)\n",
    "        I.set_description(f\"Epoch {e} (loss={l.item():.3f})\")\n",
    "        I.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d9097f-0c9b-4e5f-9056-596ecd53b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = bbb_mlp(p, X_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe222fc0-4fd9-4f90-bd53-7d675a437b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.90       980\n",
      "           1       0.84      0.99      0.91      1135\n",
      "           2       0.87      0.78      0.82      1032\n",
      "           3       0.74      0.87      0.80      1010\n",
      "           4       0.86      0.80      0.83       982\n",
      "           5       0.87      0.49      0.63       892\n",
      "           6       0.81      0.93      0.87       958\n",
      "           7       0.93      0.80      0.86      1028\n",
      "           8       0.74      0.74      0.74       974\n",
      "           9       0.77      0.78      0.77      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.83      0.82      0.81     10000\n",
      "weighted avg       0.83      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.classification_report(\n",
    "        y_test.argmax(axis=1),\n",
    "        y_hat.argmax(axis=1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0082edd-55ff-4f7f-bc3e-ba533500ea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792882105162253"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test, nn.softmax(y_hat, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018789f-d330-4b97-9d36-c288770e63bb",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "While theoretically neat, this method is both involved in terms of setup, and seems _very_ sensitive to hyper-parameter settings and intial parameters $\\theta_{t = 0}$. It is clear that the authors of this paper have spent quite a bit of time tuning these hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a6bb7-f293-46aa-8ba0-05158e71aa95",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Here I show some additional working.\n",
    "\n",
    "### KL-divergence of two univariate Gaussians\n",
    "\n",
    "We start by defining the classic KL divergence between two continuous random distributions $p(x)$ and $q(x)$:\n",
    "\n",
    "$$\n",
    "KL[p(x) || q(x)] = \\int p(x) \\log \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "and the univariate Gaussian probability (density) function:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) &= \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(- \\frac{z^2}{2}\\right) \\\\\n",
    "z &= \\frac{x - \\mu}{\\sigma}\n",
    "\\end{align}$$\n",
    "\n",
    "Taking the logarithm of the Gaussian:\n",
    "\n",
    "$$\n",
    "\\log p(x) = - \\frac{1}{2} \\left( \\log 2 \\pi + \\log \\sigma^2 + z^2 \\right)\n",
    "$$\n",
    "\n",
    "Expanding the logarithm, we get:\n",
    "\n",
    "$$\n",
    "KL[p(x) || q(x)] = \\int p(x) \\log p(x) - \\int p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "Let us first focus on the Shannon entropy (first term of RHS):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int p(x) \\log p(x) &= \\int - \\frac{1}{2} p(x) \\left( \\log 2 \\pi + \\log \\sigma^2 + z^2 \\right) \\\\\n",
    "&= - \\frac{1}{2} \\int p(x) \\left( \\log 2 \\pi + \\log \\sigma^2 + z^2 \\right) \\\\\n",
    "&= - \\frac{1}{2} \\left( \\log 2 \\pi + \\log \\sigma^2 + \\int p(x) z^2 \\right)\\\\\n",
    "&= - \\frac{1}{2} \\left( \\log 2 \\pi + \\log \\sigma^2 + \\mathbb{E}[z^2] \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Now, since\n",
    "$$\\begin{align}\n",
    "\\mathbb{V}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2 \\iff \\mathbb{E}[x^2] = \\mathbb{V}[x] + \\mathbb{E}[x]^2\n",
    "\\end{align}$$\n",
    "\n",
    "and we know that $\\mathbb{E}[z] = 0$ and $\\mathbb{V}[z] = 1$, we hence get:\n",
    "\n",
    "$$\n",
    "\\int p(x) \\log p(x) = - \\frac{1}{2} \\left( \\log 2 \\pi + \\log \\sigma^2 + 1 \\right)\n",
    "$$\n",
    "\n",
    "Next, we focus on the cross-entropy part of the KL (second term of RHS). Here, I will start introducing subscripts, such that the distinctions between $p(x)$ and $q(x)$ (and their moments) are made clear:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int p(x) \\log q(x) &= \\int - \\frac{1}{2} p(x) \\left( \\log 2 \\pi + \\log \\sigma_q^2 + z_q^2 \\right) \\\\\n",
    "&= - \\frac{1}{2} \\left( \\log 2 \\pi + \\log \\sigma_q^2 + \\mathbb{E}_p[z_q^2] \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Focusing just on $\\mathbb{E}_p[z_q^2]$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_p[z_q^2] &= \\int p(x) \\left( \\frac{x - \\mu_q}{\\sigma_q} \\right)^2 \\\\\n",
    "&= \\int p(x) \\frac{x^2 - 2x\\mu_q + \\mu_q^2}{\\sigma_q^2} \\\\\n",
    "&= \\frac{1}{\\sigma_q^2} \\left( \\int p(x) x^2  - \\int p(x) 2 x \\mu_q  + \\int p(x) \\mu_q^2 \\right) \\\\\n",
    "&= \\frac{1}{\\sigma_q^2} \\left( \\mathbb{E}[x^2] - 2 \\mu_p \\mu_q + \\mu_q^2 \\right) \\\\\n",
    "&= \\frac{1}{\\sigma_q^2} \\left( \\sigma_p^2 + \\mu_p^2 - 2 \\mu_p \\mu_q + \\mu_q^2 \\right) \\\\\n",
    "&= \\frac{1}{\\sigma_q^2} \\left( \\sigma_p^2 + (\\mu_p - \\mu_q)^2 \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c568ed-32ad-4adf-ba78-7daf9651d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
